---
title: "Exploring bupaR"
output:
  html_document:
    theme: cosmo
    toc: true
---

The below fit-to-purpose assesment is based on a few hour's of experimentation with bupaR on a real dataset numbering over 100k events. Neither computational power on larger datasets nor bahaviour around edge cases were tested. Please interpret results accordingly.

# Summary
> bupaR is a promising package covering all the standard features of process mining. Coming from academia, usage presupposes intermediate R coding skills and a basic understanding of process mining concepts. Heeding limitations of open-source solutions (lack of enterprise support, challanges of documentation and training), it nevertheless has the potential to deliver value to projects in industry.

###PROs:  
* Ease of data entry due to integration with standard R data structures
* Ease of data wranging due to conformity with tidy data analytics concepts and packages (tidyverse)
* High level of tweakability when subsetting event logs (key feature to mine interpreable process maps)
* Featurefull EDA library: ability to compute and chart on-the-point descriptive statistics
* Abllity to mine both frequency and performance maps of processes
* Ease of reporting due to RMarkdown and KnitR

###CONs:  
* Uneven maturity of features: some functionality clearly better implemented than other
* Limitations of visual outputs (interactivity, exportability, zooming)
* Limitations of accessibility: R coding required, not suited for general BA purposes
* No cloud or distributed solution: computing resource challanges for large datasets
* Lacks large user base and thus community support as yet
* Gaps in documentation: some key features lack comprehensive docs


# Notes On Install
  * diagrammeR package required to install processmapR
      + Did not work with R 3.2.x -> upgraded to R 3.4.3
      + Visualization with processmapR::process_map() did not work with CRAN versions of diagrammeR and processmpR -> worked after reinstalling dev versions from GitHub

# Data Entry: Create Event Log from CSV File

Very easy to get process data into bupaR's native S3 data structure called "event log". Documentation gives clear instruction on set-up and minimal requirements of data entry (e.g. transactional life cycles, activity instance ids etc).  

```{r setup, include=TRUE}
# Load libs
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(lubridate))

# Import event log form csv
event_log_df <-
  read.csv(here::here("Data", "event_log.csv"),
           stringsAsFactors = FALSE,
           sep = ";")

# Select life prod, add activity_instance_id, add lifecycle_id
event_log_df_life <- event_log_df %>%
  filter(PRODLINE == "life") %>%
  select(-PRODLINE) %>%
  mutate(ACTIVITY_INST_ID = as.numeric(row.names(.))) %>%
  gather(LIFECYCLE_ID,
         TIMESTAMP,
         -PROPOSALID,-USERID,
         -ACTIVITY_INST_ID,
         -EVENT) %>%
  mutate(TIMESTAMP = ymd_hms(TIMESTAMP)) %>%
  arrange(PROPOSALID, TIMESTAMP) %>%
  mutate(LIFECYCLE_ID = case_when(.$LIFECYCLE_ID == 'STARTTIMESTAMP' ~ 'START',
                                  TRUE ~ 'END'))

# Transform df into bupaR eventlog
suppressPackageStartupMessages(library(bupaR))
suppressPackageStartupMessages(library(processmapR))

event_log_life <- event_log_df_life %>%
  eventlog(
    case_id = "PROPOSALID",
    activity_id = "EVENT",
    activity_instance_id = "ACTIVITY_INST_ID",
    lifecycle_id = "LIFECYCLE_ID",
    timestamp = "TIMESTAMP",
    resource_id = "USERID"
  )
```

# Exploratory Data Analysis (EDA)

Refined EDA package available for the exploration of event logs. Most metrics are easy to plot and present visually.

## Explore Event Log Object Mappings
```{r}
event_log_life %>% mapping
```

## Summary of Event Log Object
```{r}
event_log_life %>% summary
```


## Graph of Processing Times per Event

```{r}
event_log_life %>% 
    processing_time("activity") %>%
    plot
```


## Graph of Throughput Times

This can be measured at various levels, by default here on the whole log.

```{r}
event_log_life %>% 
     throughput_time("log")%>%
    plot
```

## Measures of Structuredness

Well thought-of metrics for structural understanding of process flows. Not so directly accessible in other process mining tools.

### Activity Presence
From docs: "Activity presence shows in what percentage of cases an activity is present."

```{r}
event_log_life %>%  activity_presence() %>%
    plot
```

### Activity Frequency

```{r}
event_log_life %>%
    activity_frequency("activity")
```

### Trace Coverage
From docs: "The trace coverage metric shows the relationship between the number of different activity sequences (i.e. traces) and the number of cases they cover."

```{r}
event_log_life %>%
    trace_coverage("trace") %>%
    plot()
```


# Visualization

Both essential visualizations of process mining available: frequency and performance maps. Metrics presented on graphs are tweakable. Visual output is limited though. Complex graphs are hard to zoom in on, metrics not visible. 

## Frequency maps
```{r, warning=F}
# Filter cases having traces with at least 80% frequency
event_log_life_filt <- event_log_life %>%
  filter_trace_frequency(percentage = 0.8, reverse = F) 

# Plot frequency map
event_log_life_filt %>% 
  process_map()
```

## Performance maps
```{r, warning=F}
# Plot performance map
event_log_life_filt %>% 
  process_map(performance(median, "days"))
```

